{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+wvAmM1Rrdy9wXFUpmNmx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kk0mtl/2024-NLP/blob/main/OpenAI(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcW4Azv1rgml",
        "outputId": "89bf721e-157a-4d60-a05a-fcb4ed54ce45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.4\n",
            "    Uninstalling openai-1.54.4:\n",
            "      Successfully uninstalled openai-1.54.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tqj-6nSXr6yA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# github 저장을 위해 key는 제거하겠습니다.\n",
        "OPENAI_API_KEY = ''\n",
        "openai.api_key = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "_F-lFyt_rtKa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Completion"
      ],
      "metadata": {
        "id": "J67fp_VQkrGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_retries = 3\n",
        "retry_delay = 5"
      ],
      "metadata": {
        "id": "1aSdEFbAmSXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for attempt in range(max_retries):\n",
        "  try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model = \"gpt-3.5-turbo-0125\",\n",
        "        messages = [\n",
        "            {\"role\" : \"system\", \"content\" : \"You are a knowledgeable assistant.\"},\n",
        "            {\"role\" : \"assistant\", \"content\" : \"Answer in Korean.\"},\n",
        "            {\"role\" : \"user\", \"content\" : \"What are the health beneits of regular exercise?\"},\n",
        "        ],\n",
        "        #max_tokens = 200 # 응답이 짧아짐\n",
        "        #temperature = 0.9 # 응답의 다양성\n",
        "        seed = 42\n",
        "    )\n",
        "    print(response.choices[0].message['content'])\n",
        "    break\n",
        "  except openai.error.APIError as e:\n",
        "    print(f\"API error occurred: {e}\")\n",
        "    if attempt < max_retries - 1:\n",
        "      print(f\"Retrying in {retry_delay} seconds . . .\")\n",
        "      time.sleep(retry_delay)\n",
        "    else:\n",
        "      print(\"Max retries reached. Giving up.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tinSoKJ7kYMi",
        "outputId": "e2ab9824-e15f-4112-a02e-b37a745a55ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "규칙적인 운동은 건강에 많은 이점이 있습니다. 운동을 통해 신체적인 건강을 유지하고 강화할 수 있으며, 심혈관 건강을 개선하고 심리적인 건강에도 도움을 줄 수 있습니다. 또한 체중을 조절하고 대사를 증진시켜 면역력을 향상시키는 효과도 있습니다.운동을 통해 스트레스를 감소시키고 자기 자신에 대한 자신감을 높일 수도 있습니다. 따라서 규칙적인 운동은 건강한 삶을 유지하기 위해 중요합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "hJymqpWpsjT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts):\n",
        "  response = openai.Embedding.create(\n",
        "      input=texts,\n",
        "      model=\"text-embedding-ada-002\"\n",
        "  )\n",
        "  return [embedding['embedding'] for embedding in response['data']]\n",
        "\n",
        "# 예시 단어\n",
        "color_words = [\"red\", \"blue\", \"yellow\", \"green\", \"violet\", \"cyan\", \"black\", \"white\"]\n",
        "color_embeddings = get_embeddings(color_words)\n",
        "\n",
        "for word, embedding in zip(color_words, color_embeddings):\n",
        "  print(f\"{word} : {embedding[:5]}...\")\n",
        "  print(len(embedding))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKpoa5IrsoAi",
        "outputId": "ae883945-3072-4603-b6a1-86cafa5be52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red : [9.326533472631127e-06, -0.02476814016699791, -0.002384250983595848, -0.028791459277272224, -0.021199282258749008]...\n",
            "1536\n",
            "blue : [0.005474964156746864, -0.007486246060580015, 0.005678507499396801, -0.03110414557158947, -0.01965053379535675]...\n",
            "1536\n",
            "yellow : [0.007661858107894659, -0.024910997599363327, 0.004491548519581556, -0.02860249951481819, -0.01958620548248291]...\n",
            "1536\n",
            "green : [0.01546180434525013, -0.010975971817970276, 0.025183379650115967, -0.02092933841049671, -0.005648194346576929]...\n",
            "1536\n",
            "violet : [-0.006727131083607674, -0.018318135291337967, 0.0036361967213451862, -0.00567674869671464, -0.021194979548454285]...\n",
            "1536\n",
            "cyan : [0.021550633013248444, -0.014010688289999962, 0.008289773017168045, -0.02929886430501938, -0.016149088740348816]...\n",
            "1536\n",
            "black : [-0.015103082172572613, -0.031215764582157135, 0.00877943355590105, -0.03691864386200905, -0.01613996922969818]...\n",
            "1536\n",
            "white : [0.006292110309004784, -0.02457117661833763, 0.0002028137823799625, -0.014848269522190094, -0.0052642603404819965]...\n",
            "1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning"
      ],
      "metadata": {
        "id": "C7RhpEdKtxlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.File.create(\n",
        "    file=open(\"mydata.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "file_id = response['id']\n",
        "print(f\"Uploaded file ID : {file_id}\")"
      ],
      "metadata": {
        "id": "2-cw-XZSxqej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b246bb6-8451-47be-94e3-1fcf97bcc5a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file ID : file-4RJwq1bdNwEMXJnsMTsRpX\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 메소드 변경으로 에러 발생\n",
        "response = openai.FineTune.create(\n",
        "    training_file = file_id,\n",
        "    model = \"gpt-4o-mini-2024-07-18\"\n",
        ")\n",
        "fine_tune_id = response[\"id\"]\n",
        "print(f\"Fine-tune job ID: {fine_tune_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "6RBZinwxvUIT",
        "outputId": "bd4c90d1-2150-4bf7-c649-789b27acc0aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "Unknown request URL: POST /v1/fine-tunes. Please check the URL for typos, or see the docs at https://platform.openai.com/docs/api-reference/.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e30d5daf1ab1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = openai.FineTune.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtraining_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gpt-4o-mini-2024-07-18\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0mfine_tune_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/createable_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Unknown request URL: POST /v1/fine-tunes. Please check the URL for typos, or see the docs at https://platform.openai.com/docs/api-reference/."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune -> FineTuningJob로 변경\n",
        "response = openai.FineTuningJob.create(\n",
        "    training_file = file_id,\n",
        "    model = \"gpt-4o-mini-2024-07-18\"\n",
        ")\n",
        "fine_tune_id = response[\"id\"]\n",
        "print(f\"Fine-tune job ID: {fine_tune_id}\")"
      ],
      "metadata": {
        "id": "n3KiR_uyyyZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee61360-72f3-47f8-f0af-f976e589ed28"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tune job ID: ftjob-NXqqAUvDqSCyHSXhQVNF1tNP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  response = openai.FineTuningJob.retrieve(fine_tune_id)\n",
        "  status = response[\"status\"]\n",
        "  if status in [\"succeeded\", \"failed\"]:\n",
        "    break\n",
        "  print(f\"Fine-tune job status : {status}\")\n",
        "  time.sleep(60)"
      ],
      "metadata": {
        "id": "H_yFcllRzisb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0ca61a-6f30-4ea5-e8f1-9352826ff9ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tune job status : running\n",
            "Fine-tune job status : running\n",
            "Fine-tune job status : running\n",
            "Fine-tune job status : running\n",
            "Fine-tune job status : running\n",
            "Fine-tune job status : running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Completion 지원되지 않음\n",
        "fine_tune_job_response = openai.FineTuningJob.retrieve(fine_tune_id) # 참조할 모델 지정\n",
        "if status == 'succeeded':\n",
        "  response = openai.Completion.create(                               # Completion 지원되지 않음\n",
        "      model=fine_tune_job_response['fine_tuned_model'],\n",
        "      prompt=\"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\",\n",
        "      max_tokens = 50\n",
        "  )\n",
        "  print(f\"Fine-tuned model output : {response.choices[0].text.strip()}\")\n",
        "else:\n",
        "  print(\"Fine-tuning job failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7vomx-V4MicG",
        "outputId": "88e370a4-507d-4746-8add-d46fcc1cc2de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-734f74075277>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfine_tune_job_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFineTuningJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfine_tune_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfine_tune_job_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fine_tuned_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 접근 불가 에러 발생\n",
        "if status == 'succeeded':\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=fine_tune_id,\n",
        "      prompt=\"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\",\n",
        "      max_tokens = 50\n",
        "  )\n",
        "  print(f\"Fine-tuned model output : {response.choices[0].message['content'].strip()}\")\n",
        "else:\n",
        "  print(\"Fine-tuning job failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HzpwZUfCvrh6",
        "outputId": "0e07dbda-87d8-49ba-949f-89567e68d213"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "The model `ftjob-NXqqAUvDqSCyHSXhQVNF1tNP` does not exist or you do not have access to it.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-81f7e63cdd25>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfine_tune_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mmax_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: The model `ftjob-NXqqAUvDqSCyHSXhQVNF1tNP` does not exist or you do not have access to it."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine_tune_id -> response['fine_tuned_model']로 변경\n",
        "# 키 접근 불가 에러 발생\n",
        "if status == 'succeeded':\n",
        "  response = openai.ChatCompletion.create(\n",
        "      model=response['fine_tuned_model'],\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\"}],\n",
        "      max_tokens = 50\n",
        "  )\n",
        "  print(f\"Fine-tuned model output : {response.choices[0].message['content'].strip()}\")\n",
        "else:\n",
        "  print(\"Fine-tuning job failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xAvCLtVnLy72",
        "outputId": "af15cb2a-71a3-4723-db85-0430edb0cdac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'fine_tuned_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e31ecfab8e73>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'succeeded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   response = openai.ChatCompletion.create(\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fine_tuned_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mmax_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'fine_tuned_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_job_response = openai.FineTuningJob.retrieve(fine_tune_id) # 참조할 모델 정확히 지정\n",
        "if status == 'succeeded':\n",
        "  response = openai.ChatCompletion.create(                           # Completion -> ChatCompletion\n",
        "      model=fine_tune_job_response['fine_tuned_model'],\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Translate the following English text to French : 'Good night'\\n\\n###\\n\\n\"}],\n",
        "      max_tokens = 50\n",
        "  )\n",
        "  print(f\"Fine-tuned model output : {response.choices[0].message['content'].strip()}\")\n",
        "else:\n",
        "  print(\"Fine-tuning job failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViSIqxg2wZrT",
        "outputId": "0320e4da-5e27-450c-9472-3028b58a0459"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned model output : Bonne nuit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edit"
      ],
      "metadata": {
        "id": "fCCR8OQGwtca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_text(text, instruction):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0125\",\n",
        "        messages = [\n",
        "          {\"role\" : \"system\", \"content\" : \"You are a helpful assistant that edits text.\"},\n",
        "          {\"role\" : \"user\", \"content\" : f\"Please edit the following text : '{input_text}'. instruction : {instruction}\"},\n",
        "        ],\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "input_text = \"They meet on Sundays.\"\n",
        "instruction = \"Change 'Sunday' to 'Friday' and change the tense to past perfect.\" # 월요일 -> 금요일, 과거 -> 과거완료\n",
        "edited_text = edit_text(input_text, instruction)\n",
        "print(f\"Edited text : {edited_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kks79k7yr-KZ",
        "outputId": "6b94cf8f-54dd-4b89-c036-dcc75abce8ec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edited text : The corrected text is: 'They had met on Fridays.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moderation"
      ],
      "metadata": {
        "id": "tf1GWv47w-8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moderate_text(input_text):\n",
        "  response = openai.Moderation.create(\n",
        "    input=input_text,\n",
        "    model = \"text-moderation-stable\"\n",
        "  )\n",
        "  return response\n",
        "\n",
        "input_texts = [\n",
        "    \"If you want to make some quick cash, here’s how you can get away with robbing a bank.\",\n",
        "    \"I don’t care about other people’s problems, as long as they stay out of my way. I’d be happy if some people just disappeared\",\n",
        "    \"Sometimes I wonder if the world would be better off if we just wiped out a few people. Just a thought.\",\n",
        "    \"maybe I should just end it all. It’s not like anyone would miss me anyway.\",\n",
        "    \"I want to harm myself\",\n",
        "    \"You are an amazing person!\",\n",
        "    \"Let's meet at 8 PM\",\n",
        "    \"I hate you and I want to hurt you\",\n",
        "]\n",
        "\n",
        "for text in input_texts:\n",
        "  moderation_result = moderate_text(text)\n",
        "  print(f\"Input : {text}\")\n",
        "  print(f\"Moderation Result : {moderation_result}\")\n",
        "  print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMSHx4bHtR4h",
        "outputId": "70f4c27e-1981-4a3a-81f0-3071240c12a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : If you want to make some quick cash, here’s how you can get away with robbing a bank.\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsOAQTp01pUfaBTYLg8u8D8TVkT\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 1.997536855924409e-05,\n",
            "        \"hate\": 5.109478024678538e-06,\n",
            "        \"harassment\": 0.0018051965162158012,\n",
            "        \"self-harm\": 0.000177246707607992,\n",
            "        \"sexual/minors\": 3.1252536700776545e-06,\n",
            "        \"hate/threatening\": 9.644687679610797e-07,\n",
            "        \"violence/graphic\": 3.47007044183556e-05,\n",
            "        \"self-harm/intent\": 2.0884795958409086e-05,\n",
            "        \"self-harm/instructions\": 9.794011930353008e-06,\n",
            "        \"harassment/threatening\": 0.003570430912077427,\n",
            "        \"violence\": 0.15816925466060638\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : I don’t care about other people’s problems, as long as they stay out of my way. I’d be happy if some people just disappeared\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsOeRo7ao3ZTWFNUHWwfD5TiOPY\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 7.0305254666891415e-06,\n",
            "        \"hate\": 0.11762282997369766,\n",
            "        \"harassment\": 0.2775837481021881,\n",
            "        \"self-harm\": 0.006020050961524248,\n",
            "        \"sexual/minors\": 2.4680568913026946e-06,\n",
            "        \"hate/threatening\": 0.001019880292005837,\n",
            "        \"violence/graphic\": 3.5146182199241593e-05,\n",
            "        \"self-harm/intent\": 0.004867311101406813,\n",
            "        \"self-harm/instructions\": 0.00015793951752129942,\n",
            "        \"harassment/threatening\": 0.18631397187709808,\n",
            "        \"violence\": 0.29659464955329895\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : Sometimes I wonder if the world would be better off if we just wiped out a few people. Just a thought.\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsOIzJuWKpAxsYIOHH1WuLJlkSa\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": true,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": true,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": true\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 2.8791275781259174e-06,\n",
            "        \"hate\": 0.6272444725036621,\n",
            "        \"harassment\": 0.3863442540168762,\n",
            "        \"self-harm\": 4.6002947783563286e-05,\n",
            "        \"sexual/minors\": 1.5850413603857305e-07,\n",
            "        \"hate/threatening\": 0.3618997633457184,\n",
            "        \"violence/graphic\": 0.0001595159701537341,\n",
            "        \"self-harm/intent\": 3.3147302019642666e-05,\n",
            "        \"self-harm/instructions\": 7.0473079176736064e-06,\n",
            "        \"harassment/threatening\": 0.2675938904285431,\n",
            "        \"violence\": 0.988389253616333\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : maybe I should just end it all. It’s not like anyone would miss me anyway.\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsPupMWXcYbjQzXfQNWxwZkiBWG\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": true,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": true,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 2.170724883399089e-06,\n",
            "        \"hate\": 1.3623808627016842e-05,\n",
            "        \"harassment\": 0.004188804421573877,\n",
            "        \"self-harm\": 0.9643160104751587,\n",
            "        \"sexual/minors\": 2.9419393854368536e-07,\n",
            "        \"hate/threatening\": 1.1067130571973394e-06,\n",
            "        \"violence/graphic\": 0.0003928588703274727,\n",
            "        \"self-harm/intent\": 0.9582923054695129,\n",
            "        \"self-harm/instructions\": 0.00042971549555659294,\n",
            "        \"harassment/threatening\": 0.0013902441132813692,\n",
            "        \"violence\": 0.01631571352481842\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : I want to harm myself\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsPw9JFgQAd65FlB4RDF3bO5o3H\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": true,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": true,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 4.5077558752382174e-05,\n",
            "        \"hate\": 1.677227373875212e-05,\n",
            "        \"harassment\": 9.058971772901714e-05,\n",
            "        \"self-harm\": 0.9851293563842773,\n",
            "        \"sexual/minors\": 7.025177637842717e-06,\n",
            "        \"hate/threatening\": 8.685567991051357e-06,\n",
            "        \"violence/graphic\": 1.6436490113846958e-05,\n",
            "        \"self-harm/intent\": 0.9828479290008545,\n",
            "        \"self-harm/instructions\": 0.0003516814031172544,\n",
            "        \"harassment/threatening\": 4.188936509308405e-05,\n",
            "        \"violence\": 0.0056501454673707485\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : You are an amazing person!\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsP3O2kJ8SokQCGwDjAfKQ051cD\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 1.4516678675136063e-05,\n",
            "        \"hate\": 2.0567363208101597e-06,\n",
            "        \"harassment\": 8.549752237740904e-05,\n",
            "        \"self-harm\": 1.0583437415334629e-06,\n",
            "        \"sexual/minors\": 7.804217005968894e-08,\n",
            "        \"hate/threatening\": 1.4541080561869535e-09,\n",
            "        \"violence/graphic\": 6.745933660567971e-07,\n",
            "        \"self-harm/intent\": 1.034746560435451e-06,\n",
            "        \"self-harm/instructions\": 5.781692834716523e-06,\n",
            "        \"harassment/threatening\": 2.2142722855278407e-07,\n",
            "        \"violence\": 8.049375537666492e-06\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : Let's meet at 8 PM\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsQ6dHxbnBcdI3LS3oH28YE7jol\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": false,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": false\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 5.8752681070473045e-05,\n",
            "        \"hate\": 0.00012885418254882097,\n",
            "        \"harassment\": 0.00015181978233158588,\n",
            "        \"self-harm\": 1.92907882592408e-05,\n",
            "        \"sexual/minors\": 8.138279554259498e-06,\n",
            "        \"hate/threatening\": 0.00023967931338120252,\n",
            "        \"violence/graphic\": 3.22421983582899e-05,\n",
            "        \"self-harm/intent\": 2.443945049890317e-05,\n",
            "        \"self-harm/instructions\": 6.993973329372238e-07,\n",
            "        \"harassment/threatening\": 0.002505103126168251,\n",
            "        \"violence\": 0.0017632842063903809\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n",
            "Input : I hate you and I want to hurt you\n",
            "Moderation Result : {\n",
            "  \"id\": \"modr-AWgsQG3lFUUW0xKzwrNZKhzp54nyh\",\n",
            "  \"model\": \"text-moderation-007\",\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"flagged\": true,\n",
            "      \"categories\": {\n",
            "        \"sexual\": false,\n",
            "        \"hate\": false,\n",
            "        \"harassment\": false,\n",
            "        \"self-harm\": false,\n",
            "        \"sexual/minors\": false,\n",
            "        \"hate/threatening\": false,\n",
            "        \"violence/graphic\": false,\n",
            "        \"self-harm/intent\": false,\n",
            "        \"self-harm/instructions\": false,\n",
            "        \"harassment/threatening\": false,\n",
            "        \"violence\": true\n",
            "      },\n",
            "      \"category_scores\": {\n",
            "        \"sexual\": 6.720340752508491e-05,\n",
            "        \"hate\": 0.0006095448625274003,\n",
            "        \"harassment\": 0.3736913502216339,\n",
            "        \"self-harm\": 0.0011490226024761796,\n",
            "        \"sexual/minors\": 1.4773519296795712e-07,\n",
            "        \"hate/threatening\": 5.821671584271826e-05,\n",
            "        \"violence/graphic\": 8.273184357676655e-05,\n",
            "        \"self-harm/intent\": 0.000925797619856894,\n",
            "        \"self-harm/instructions\": 0.00010909802949754521,\n",
            "        \"harassment/threatening\": 0.20684465765953064,\n",
            "        \"violence\": 0.8731279373168945\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images"
      ],
      "metadata": {
        "id": "uEFWsOdBxEsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def generate_image(prompt):\n",
        "  response = openai.Image.create(\n",
        "    prompt=prompt,\n",
        "    n = 1,\n",
        "    size = \"1024x1024\"\n",
        "  )\n",
        "  image_url = response['data'][0]['url']\n",
        "  return image_url"
      ],
      "metadata": {
        "id": "TRCFzf4t0hRz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image(image_url, filename):\n",
        "  response = requests.get(image_url)\n",
        "  img = Image.open(BytesIO(response.content))\n",
        "  img.save(filename)\n",
        "\n",
        "prompt = \"A cozy, rustic vilage, with warm light emanation from windows, in the style of Van Gogh, Bird's eye view\"\n",
        "\n",
        "image_url = generate_image(prompt)\n",
        "print(f\"Image URL : {image_url}\")\n",
        "\n",
        "save_image(image_url, \"generated_image.png\")\n",
        "print(\"Image saved as generated_image.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jrAbahW1T3i",
        "outputId": "951948dc-98d1-45b5-bf5f-763e72e18b21"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL : https://oaidalleapiprodscus.blob.core.windows.net/private/org-Q5siHWWyOhMUvasJJ9FGOvVf/capstone-rooms/img-B7N74z2D8U1sFWt4FMOtmbOv.png?st=2024-11-23T08%3A45%3A05Z&se=2024-11-23T10%3A45%3A05Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-11-23T00%3A09%3A43Z&ske=2024-11-24T00%3A09%3A43Z&sks=b&skv=2024-08-04&sig=8zzVO973qB5vjhUC/5ysXNL3rcFX%2B6MSckDBpHtaJn0%3D\n",
            "Image saved as generated_image.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cat():\n",
        "  response = openai.Image.create(\n",
        "    model = \"dall-e-3\",\n",
        "    prompt = \"a white siamese cat\",\n",
        "    quality = \"standard\",\n",
        "    n = 1\n",
        "  )\n",
        "  image_url = response['data'][0]['url']\n",
        "  return image_url\n",
        "\n",
        "image_url = generate_cat()\n",
        "print(f\"Image URL : {image_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKeQiFQl26Ki",
        "outputId": "bf0f6e25-fb55-477e-d9a9-2547873ef084"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image URL : https://oaidalleapiprodscus.blob.core.windows.net/private/org-Q5siHWWyOhMUvasJJ9FGOvVf/capstone-rooms/img-WznVK6H8OS7wZ4ahbjPcz083.png?st=2024-11-23T08%3A45%3A35Z&se=2024-11-23T10%3A45%3A35Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-11-23T00%3A13%3A07Z&ske=2024-11-24T00%3A13%3A07Z&sks=b&skv=2024-08-04&sig=IaHlD8rDsFpgXJfOVPCPziD%2BloxXFrEzEVSRO6lM%2BYQ%3D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Codex"
      ],
      "metadata": {
        "id": "FlqMCp4ZxKp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_code(prompt, model=\"gpt-3.5-turbo-instruct\", max_tokens=1000):\n",
        "  try:\n",
        "    response = openai.Completion.create(\n",
        "      model = model,\n",
        "      prompt = prompt,\n",
        "      max_tokens = max_tokens,\n",
        "      temperature=0,\n",
        "      n=1,\n",
        "      stop=None,\n",
        "    )\n",
        "    code = response.choices[0].text.strip()\n",
        "    return code\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred : {str(e)}\")\n",
        "    return None\n",
        "\n",
        "prompt = \"Write a C code that computes Fibonacci number using memoization\"\n",
        "\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "if generated_code:\n",
        "  print(\"Generated Code : \\n\")\n",
        "  print(generated_code)\n",
        "else:\n",
        "  print(\"Failed to generate code\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnbXQh0P-MIZ",
        "outputId": "6c7a33c4-612e-4901-bc51-9c5761067c9f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Code : \n",
            "\n",
            "#include <stdio.h>\n",
            "\n",
            "// Function to calculate Fibonacci number using memoization\n",
            "int fib(int n, int memo[])\n",
            "{\n",
            "    // Base cases\n",
            "    if (n == 0 || n == 1)\n",
            "        return n;\n",
            "\n",
            "    // Check if the value is already calculated\n",
            "    if (memo[n] != -1)\n",
            "        return memo[n];\n",
            "\n",
            "    // Calculate and store the value in the memo array\n",
            "    memo[n] = fib(n - 1, memo) + fib(n - 2, memo);\n",
            "\n",
            "    // Return the calculated value\n",
            "    return memo[n];\n",
            "}\n",
            "\n",
            "int main()\n",
            "{\n",
            "    int n;\n",
            "    printf(\"Enter the value of n: \");\n",
            "    scanf(\"%d\", &n);\n",
            "\n",
            "    // Initialize the memo array with -1\n",
            "    int memo[n + 1];\n",
            "    for (int i = 0; i <= n; i++)\n",
            "        memo[i] = -1;\n",
            "\n",
            "    // Call the fib function and print the result\n",
            "    printf(\"Fibonacci number at position %d is %d\", n, fib(n, memo));\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "/*\n",
            "Output:\n",
            "\n",
            "Enter the value of n: 6\n",
            "Fibonacci number at position 6 is 8\n",
            "*/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File"
      ],
      "metadata": {
        "id": "S5L2uSkGxOza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upload_response = openai.File.create(\n",
        "    file=open(\"king-style-chat.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "print(\"Upload Response : \")\n",
        "print(upload_response)\n",
        "\n",
        "list_response = openai.File.list()\n",
        "print(\"List Response : \")\n",
        "print(list_response)\n",
        "\n",
        "file_id = upload_response['id']\n",
        "retrieve_response = openai.File.retrieve(file_id)\n",
        "print(\"Retrieve Response : \")\n",
        "print(retrieve_response)\n",
        "\n",
        "delete_response = openai.File.delete(file_id)\n",
        "print(\"Delete Response : \")\n",
        "print(delete_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFULDzWy5U1q",
        "outputId": "ccdb0aea-1b22-4683-d7b2-d0a934c2b3ec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Response : \n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-489cXRiQxisvqW3dkbVRgV\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 57615,\n",
            "  \"created_at\": 1732355232,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n",
            "List Response : \n",
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-489cXRiQxisvqW3dkbVRgV\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 57615,\n",
            "      \"created_at\": 1732355232,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-STvyaXCiZEJed9wgciHhCJ\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2184,\n",
            "      \"created_at\": 1732351052,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-4RJwq1bdNwEMXJnsMTsRpX\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1732350686,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-pOr8gTMFcB5y0awZd2ykgDJj\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2164,\n",
            "      \"created_at\": 1732195294,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-rWNV75acUyOSizwyV4XpzEau\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2144,\n",
            "      \"created_at\": 1732195294,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-OkHfIrg6hrhgUEOCHv3xRB1Y\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1732194920,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-GJK81HtTVXV5NYCgQ37TFYwE\",\n",
            "      \"purpose\": \"fine-tune-results\",\n",
            "      \"filename\": \"step_metrics.csv\",\n",
            "      \"bytes\": 2188,\n",
            "      \"created_at\": 1732193381,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-4Ul8Z1ChUE4AFDi9740GESMc\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 3128,\n",
            "      \"created_at\": 1732192821,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    },\n",
            "    {\n",
            "      \"object\": \"file\",\n",
            "      \"id\": \"file-U36E2q1S4EvJT0vvzKIWTvb8\",\n",
            "      \"purpose\": \"fine-tune\",\n",
            "      \"filename\": \"file\",\n",
            "      \"bytes\": 57615,\n",
            "      \"created_at\": 1731893675,\n",
            "      \"status\": \"processed\",\n",
            "      \"status_details\": null\n",
            "    }\n",
            "  ],\n",
            "  \"has_more\": false,\n",
            "  \"first_id\": \"file-489cXRiQxisvqW3dkbVRgV\",\n",
            "  \"last_id\": \"file-U36E2q1S4EvJT0vvzKIWTvb8\"\n",
            "}\n",
            "Retrieve Response : \n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"id\": \"file-489cXRiQxisvqW3dkbVRgV\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"filename\": \"file\",\n",
            "  \"bytes\": 57615,\n",
            "  \"created_at\": 1732355232,\n",
            "  \"status\": \"processed\",\n",
            "  \"status_details\": null\n",
            "}\n",
            "Delete Response : \n",
            "{\n",
            "  \"object\": \"file\",\n",
            "  \"deleted\": true,\n",
            "  \"id\": \"file-489cXRiQxisvqW3dkbVRgV\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio"
      ],
      "metadata": {
        "id": "vXcP2_77xRJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(file_path, model=\"whisper-1\", response_format=\"json\", temperature=0.1, language=None, prompt=None):\n",
        "  with open(file_path, \"rb\") as audio_file:\n",
        "    response = openai.Audio.transcribe(\n",
        "        file=audio_file,\n",
        "        model=model,\n",
        "        response_format=response_format,\n",
        "        temperature=temperature,\n",
        "        language=language,\n",
        "        prompt=prompt\n",
        "    )\n",
        "  return response\n",
        "\n",
        "file_path = \"les-miserables.mp3\"\n",
        "\n",
        "transcription = transcribe_audio(file_path)\n",
        "print(\"Transcription Response :\")\n",
        "print(transcription)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T6XjWPD_NOo",
        "outputId": "114a394e-02dd-4a6d-d867-28b6fbbf0eb8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription Response :\n",
            "{\n",
            "  \"text\": \"Do you hear the people sing, singing the song of angry men? It is the music of a people who will not be slaves again. When the beating of your heart echoes the beating of the drums, there is a life about to start when tomorrow comes. Will you join in our crusade? Who will be strong and stand with me? Beyond the barricade, is there a world you long to see? Then join in the fight that will give you the right to be free. Do you hear the people sing, singing the song of angry men? It is the music of a people who will not be slaves again. When the beating of your heart echoes the beating of the drums, there is a life about to start when tomorrow comes. Will you give all you can give so that our banners may advance? Some will fall and some will lift. Will you stand up and take your chance? The blood of the martyrs will fall to the meadows of Christ. Do you hear the people sing, singing the song of angry men? It is the music of a people who will not be slaves again. When the beating of your heart echoes the beating of the drums, there is a life about to start when tomorrow comes.\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}